# IFRI-AI-Chatbot

Un chatbot intelligent d√©velopp√© pour l'Institut de Formation et de Recherche en Informatique (IFRI) qui utilise l'intelligence artificielle pour r√©pondre aux questions des √©tudiants et du personnel concernant l'institution.

## üìã Table des mati√®res

- [Aper√ßu du projet](#aper√ßu-du-projet)
- [Architecture technique](#architecture-technique)
- [Pr√©requis](#pr√©requis)
- [Installation](#installation)
- [D√©marrage](#d√©marrage)
- [Fonctionnement du code](#fonctionnement-du-code)
- [Structure du projet](#structure-du-projet)
- [Configuration](#configuration)
- [Utilisation](#utilisation)
- [Contribuer](#contribuer)

## Aper√ßu du projet

IFRI-AI-Chatbot est une application Streamlit qui utilise un mod√®le de langage avanc√©s (LLM) et des techniques de Retrieval-Augmented Generation (RAG) pour fournir des r√©ponses pr√©cises et contextuelles aux questions concernant l'Institut de Formation et de Recherche en Informatique (IFRI). Le chatbot peut traiter des documents PDF contenant des informations institutionnelles et g√©n√©rer des r√©ponses pertinentes en fran√ßais.


### 1. Retrieval-Augmented Generation (RAG)

Le syst√®me utilise l'approche RAG qui combine :
- **Retrieval** : Recherche d'informations pertinentes dans une base de donn√©es vectorielle
- **Generation** : G√©n√©ration de r√©ponses contextuelles avec un mod√®le de langage

### 2. Pipeline de traitement

```mermaid
graph TD
    A[Documents PDF] --> B[Extraction de texte]
    B --> C[Segmentation en chunks]
    C --> D[Cr√©ation d'embeddings]
    D --> E[Base vectorielle Chroma]
    E --> F[Recherche de similarit√©]
    F --> G[Contexte pertinent]
    G --> H[LLM Gemini]
    H --> I[R√©ponse g√©n√©r√©e]
```

### 3. Techniques d'embeddings

- **Mod√®le utilis√©** : `sentence-transformers/all-MiniLM-L12-v2` via HuggingFace
- **Dimensionnalit√©** : Vecteurs de haute dimension pour capturer la s√©mantique
- **Similarit√© cosinus** : Pour trouver les documents les plus pertinents

### 4. Gestion de la m√©moire conversationnelle

- **ConversationBufferMemory** : Maintient l'historique des conversations
- **Question standalone** : Reformulation des questions de suivi pour inclure le contexte
- **Compression contextuelle** : Optimisation de la r√©cup√©ration d'informations

## Architecture technique

### Composants principaux

1. **Interface utilisateur** : Streamlit
2. **Mod√®le de langage** : Google Gemini 1.5 Pro
3. **Embeddings** : HuggingFace Inference API
4. **Base vectorielle** : ChromaDB
5. **Framework** : LangChain

### Technologies utilis√©es

- **Python 3.8+**
- **Streamlit** : Interface web interactive
- **LangChain** : Framework pour applications LLM
- **ChromaDB** : Base de donn√©es vectorielle
- **Google Gemini** : Mod√®le de langage conversationnel
- **HuggingFace** : API d'embeddings
- **PyPDF** : Extraction de texte des PDF

## Pr√©requis

- Python 3.8 ou sup√©rieur
- Cl√© API Google Gemini
- Cl√© API HuggingFace
- Connexion internet pour les API
- Au moins 4 GB de RAM

## Installation

### 1. Cloner le repository

```bash
git clone https://github.com/Arix-ALIMAGNIDOKPO/IFRI-AI-Chatbot.git
cd IFRI-AI-Chatbot
```

### 2. Cr√©er un environnement virtuel

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

### 3. Installer les d√©pendances

```bash
pip install -r requirements.txt
```

### 4. Configuration des cl√©s API

Remplacez les cl√©s API dans le fichier `Chatbot.py` :

```python
# Ligne 65 - Cl√© API Google Gemini
google_api_key="VOTRE_CLE_API_GOOGLE"

# Ligne 74 - Cl√© API HuggingFace
api_key="VOTRE_CLE_API_HUGGINGFACE"
```

> **S√©curit√©** : Pour un d√©ploiement en production, utilisez des variables d'environnement pour stocker les cl√©s API.

## D√©marrage

### 1. Lancement de l'application

```bash
streamlit run Chatbot.py
```

### 2. Acc√®s √† l'interface

L'application sera accessible √† l'adresse : `http://localhost:8501`

### 3. Premi√®re utilisation

Au premier lancement, le syst√®me :
1. Charge et traite tous les documents PDF du dossier `Documents/`
2. Cr√©e les embeddings vectoriels
3. Initialise la base de donn√©es ChromaDB
4. Configure le mod√®le conversationnel

## Fonctionnement du code

### 1. Chargement et pr√©paration des documents

```python
# Chargement des PDF depuis le dossier Documents
loader = PyPDFDirectoryLoader("Documents")
documents = loader.load()

# Segmentation en chunks pour optimiser la recherche
text_splitter = RecursiveCharacterTextSplitter(
    separators = ["\n\n", "\n", " ", ""],
    chunk_size = 1600,      # Taille optimale pour les embeddings
    chunk_overlap= 200      # Chevauchement pour maintenir le contexte
)
chunks = text_splitter.split_documents(documents=documents)
```

### 2. Cr√©ation des embeddings et de la base vectorielle

```python
def select_embeddings_model(LLM_service="HuggingFace"):
    """S√©lection du mod√®le d'embeddings via HuggingFace API"""
    embeddings = HuggingFaceInferenceAPIEmbeddings(
        api_key="VOTRE_CLE_API",
        model_name="sentence-transformers/all-MiniLM-L12-v2"
    )
    return embeddings

# Cr√©ation de la base vectorielle persistante
vector_store = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="vector_store/Ifri"
)
```

### 3. Configuration du retriever

```python
def Vectorstore_backed_retriever(vectorstore, search_type="similarity", k=5):
    """Configuration du syst√®me de r√©cup√©ration par similarit√©"""
    retriever = vectorstore.as_retriever(
        search_type=search_type,
        search_kwargs={'k': k}  # Retourne les 5 documents les plus pertinents
    )
    return retriever
```

### 4. Cha√Æne conversationnelle personnalis√©e

```python
def custom_ConversationalRetrievalChain(llm, condense_question_llm, retriever, language="french"):
    """
    Cr√©ation d'une cha√Æne conversationnelle en 2 √©tapes :
    1. Reformulation de la question avec le contexte historique
    2. R√©cup√©ration de documents et g√©n√©ration de r√©ponse
    """
    
    # √âtape 1 : Question standalone
    standalone_question_chain = {
        "standalone_question": {
            "question": lambda x: x["question"],
            "chat_history": lambda x: get_buffer_string(x["chat_history"]),
        }
        | condense_question_prompt
        | condense_question_llm
        | StrOutputParser(),
    }
    
    # √âtape 2 : R√©cup√©ration et g√©n√©ration
    retrieved_documents = {
        "docs": itemgetter("standalone_question") | retriever,
        "question": lambda x: x["standalone_question"],
    }
    
    return conversational_retriever_chain, memory
```

### 5. Interface Streamlit

```python
# Configuration de l'interface utilisateur
st.image('ifri.png', width=200)
st.caption("Bienvenue sur le chatbot de l'IFRI !")

# Gestion des messages et de l'historique
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "En quoi pouvons nous vous aider ?"}]

# Traitement des questions utilisateur
if prompt := st.chat_input():
    with st.spinner('Traitement de la question en cours...'):
        response = chain_gemini.invoke({"question": prompt})
        
    # Sauvegarde dans la m√©moire conversationnelle
    memory_gemini.save_context({"question": prompt}, {"answer": response['answer'].content})
```

## üìÅ Structure du projet

```
IFRI-AI-Chatbot/
‚îÇ
‚îú‚îÄ‚îÄ Chatbot.py                 # Application principale Streamlit
‚îú‚îÄ‚îÄ requirements.txt           # D√©pendances Python
‚îú‚îÄ‚îÄ packages.txt              # Packages syst√®me (sqlite3)
‚îú‚îÄ‚îÄ ifri.png                  # Logo de l'IFRI
‚îú‚îÄ‚îÄ README.md                 # Documentation (ce fichier)
‚îÇ
‚îú‚îÄ‚îÄ Documents/                # Dossier des documents source
‚îÇ   ‚îú‚îÄ‚îÄ AOF.pdf
‚îÇ   ‚îú‚îÄ‚îÄ Conditions de d√©livrance des actes acad√©miques (2).pdf
‚îÇ   ‚îú‚îÄ‚îÄ Offre_de_formation_licence_ifri (2).pdf
‚îÇ   ‚îú‚îÄ‚îÄ REGLEMENT INTERIEUR.pdf
‚îÇ   ‚îî‚îÄ‚îÄ SiteWebScraping (2).pdf
‚îÇ
‚îî‚îÄ‚îÄ vector_store/             # Base de donn√©es vectorielle (g√©n√©r√©e automatiquement)
    ‚îî‚îÄ‚îÄ Ifri/
        ‚îú‚îÄ‚îÄ chroma.sqlite3
        ‚îî‚îÄ‚îÄ ...
```

## ‚öôÔ∏è Configuration

### Variables importantes √† modifier

1. **Cl√©s API** (dans `Chatbot.py`) :
   ```python
   # Google Gemini API Key
   google_api_key="VOTRE_CLE_API_GOOGLE"
   
   # HuggingFace API Key
   api_key="VOTRE_CLE_API_HUGGINGFACE"
   ```

2. **Param√®tres de segmentation** :
   ```python
   chunk_size = 1600      # Taille des segments (tokens)
   chunk_overlap = 200    # Chevauchement entre segments
   ```

3. **Param√®tres de r√©cup√©ration** :
   ```python
   k = 5                  # Nombre de documents √† r√©cup√©rer
   temperature = 0.5      # Cr√©ativit√© du mod√®le (0 = d√©terministe, 1 = cr√©atif)
   ```

### Configuration pour le d√©ploiement

Pour un d√©ploiement s√©curis√©, cr√©ez un fichier `.env` :

```bash
GOOGLE_API_KEY=votre_cle_api_google
HUGGINGFACE_API_KEY=votre_cle_api_huggingface
```

Et modifiez le code pour utiliser `os.getenv()`.

## üéØ Utilisation

### 1. Types de questions support√©es

- **Questions g√©n√©rales** : "Qu'est-ce que l'IFRI ?"
- **Informations acad√©miques** : "Quelles sont les conditions d'admission ?"
- **R√®glement** : "Quel est le r√®glement int√©rieur ?"
- **Formations** : "Quelles formations sont disponibles ?"
- **Questions de suivi** : Le chatbot maintient le contexte conversationnel

### 2. Fonctionnalit√©s

- **R√©ponses contextuelles** : Bas√©es sur les documents officiels de l'IFRI
- **M√©moire conversationnelle** : Comprend les questions de suivi
- **Interface intuitive** : Chat en temps r√©el avec Streamlit
- **Reset de conversation** : Bouton pour recommencer une nouvelle conversation

### 3. Limitations

- **Langue** : Optimis√© pour le fran√ßais
- **Source** : Limit√© aux documents fournis dans le dossier `Documents/`
- **Connexion** : N√©cessite une connexion internet pour les API
- **Tokens** : Limit√© par les quotas des API Google et HuggingFace

## üîÑ Workflow de traitement d'une question

1. **R√©ception** : L'utilisateur saisit une question
2. **Reformulation** : Si c'est une question de suivi, elle est reformul√©e avec le contexte
3. **Recherche vectorielle** : Recherche des documents les plus pertinents via embeddings
4. **Construction du prompt** : Combinaison du contexte, de l'historique et de la question
5. **G√©n√©ration** : Le mod√®le Gemini g√©n√®re une r√©ponse
6. **Affichage** : La r√©ponse est affich√©e √† l'utilisateur
7. **M√©morisation** : L'√©change est sauvegard√© dans la m√©moire conversationnelle

## Optimisations possibles

### Performance
- Mise en cache des embeddings
- Utilisation de mod√®les locaux pour r√©duire la latence
- Optimisation de la taille des chunks

### Fonctionnalit√©s
- Support multilingue
- Upload dynamique de documents
- Syst√®me de feedback utilisateur
- Analytics des conversations

### S√©curit√©
- Authentification utilisateur
- Chiffrement des donn√©es sensibles
- Audit des conversations

## ü§ù Contribuer

1. Forkez le projet
2. Cr√©ez une branche pour votre fonctionnalit√© (`git checkout -b feature/AmazingFeature`)
3. Committez vos changements (`git commit -m 'Add some AmazingFeature'`)
4. Poussez vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrez une Pull Request


**D√©velopp√© avec ‚ù§Ô∏è pour l'IFRI**

[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://github.com/Arix-ALIMAGNIDOKPO/IFRI-AI-Chatbot)